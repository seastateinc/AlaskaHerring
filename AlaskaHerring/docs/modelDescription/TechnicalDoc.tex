\documentclass[12pt,letterpaper]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}
\usepackage{lscape}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% Multicols
\usepackage{multicol}
\setlength{\columnseprule}{0.5pt}
\setlength{\columnsep}{15pt}

% More symbols
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{bm}

% Surround parts of graphics with box
\usepackage{boxedminipage}

% Longtables
\usepackage{longtable}

% Package for including code in the document
\usepackage{listings}
\usepackage{alltt}

% If you want to generate a toc for each chapter (use with book)
% \usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

% Natbib
\usepackage[round]{natbib}


%% -math-
\def\bs#1{\boldsymbol{#1}}

\newcounter{saveEq}
  \def\putEq{\setcounter{saveEq}{\value{equation}}}
  \def\getEq{\setcounter{equation}{\value{saveEq}}}
  \def\tableEq{ % equations in tables
    \putEq \setcounter{equation}{0}
    \renewcommand{\theequation}{T\arabic{table}.\arabic{equation}}
    \vspace{-5mm}
    }
  \def\normalEq{ % renew normal equations
    \getEq
    \renewcommand{\theequation}{\arabic{section}.\arabic{equation}}}

  \def\puthrule{ %thick rule lines for equation tables
    \hrule \hrule \hrule \hrule \hrule}

% Hyperref
% \usepackage{url}
\usepackage[colorlinks,bookmarks,citecolor=magenta,linkcolor=blue]{hyperref}
% \usepackage{hyperref}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi


% \usepackage{tikz-uml}


\title{Age-structured model for Alaska herring stocks}
\author{Steve Martell}



% my macros
\newcommand{\fspr}{$F_{\textnormal{SPR}}$}
\newcommand{\bspr}{$B_{\textnormal{SPR\%}}$}

\newcommand{\fmsy}{$F_{\textnormal{MSY}}$}
\newcommand{\bmsy}{$B_{\textnormal{MSY}}$}

\newcommand{\ham}{HAM}


\begin{document}
  \maketitle
  \renewcommand{\abstractname}{Executive Summary}
  \begin{abstract}
    This document describes the proposed changes that have been made to the Age-structured assessment model for Alaska herring stocks. This was a contract project between the State of Alaska's Department of Fish and Game and Steven Martell at Sea State Inc.

    The objective of this project was to review and modify the existing AD Model Builder Code for the Age-structured model for Alaska herring stocks (version 0.1 Jan 2015).  The overarching objective of the modifications are:  to improve numerical stability, ease of use, general flexibility for alternative structural assumptions, and estimation of observation and process error variance to better quantify uncertainty. The following list of bullets summarizes the proposed changes that have been implemented to date:

    \begin{itemize}
      \item Modifications to the Input Data File.  Users can now specify estimates of observation error for each annual observation for: catch, egg surveys, mile milt days, and composition data.
      \item Modifications to the Control file.  Changes to the control file now allow users to estimate or fix parameters, change the phase of estimation, set initial parameter values, apply informative priors of various statistical distributions, all without having to recompile the code.  This permits rapid exploration (even automated) of alternative hypotheses and structural assumptions that are repeatable.
      \item Added controls for the addition of time varying natural mortality rates, blocks of time-varying maturity, a flexible system from implementing a wide variety of selectivity options including time-varying blocks, or continuous non-parametric functions (i.e., cubic splines). The control file is also structured so it can expand with new model features, or custom outputs, that develop in the future.
      \item Custom command line options were added to the code. Two options were added to permit rapid simulation testing (\texttt{-sim} option), and automate the procedures of conducting retrospective analysis without having to make any potentially dangerous modifications to input files (the \texttt{-retro }option). 
      \item Many of the previous routines in the current version of the stock assessment model have been broken down into smaller functions.  This both reduces the amount of redundant code that currently exists and makes the code easier to read and understand by humans.
      \item The model has 5 major components:
        \begin{enumerate}
          \item Inputs (includes data and controls that specifies model structure).
          \item Population dynamics: a collection of sub-models that relate to the biology (e.g., natural mortality, maturity, stock-recruitment).
          \item Observation dynamics: a collection of sub-models that relate how fishing mortality interacts with population model (e.g., fisheries selectivity, fishing mortality, predicted egg abundance index, predicted composition data).
          \item Statistical criterion: the objective function that relates estimated model parameters to differences between observed and predicted variables.
          \item Outputs; including and not limited to parameter estimates, convergence criterion, derived management quantities and residuals.
        \end{enumerate}
      \item There are a few structural differences being proposed in this model that relate to how selectivity is modeled, the observation error assumed in the composition data, and variance terms that relate to both process error and observation error.
        \begin{itemize}
          \item To avoid breaking the derivative chain in calculating the objective function and its gradient, use of the max function to re-scale the selectivities should be avoided.  Often you can get away with it in very simple models where selectivity is very well informed, but can soon become problematic when your jointly estimating additional parameters that are confounded with selectivity (e.g., time-varying natural mortality).  To do so, the proposed change rescales the selectivity vector for ages such that it has a mean of 1. 
          \item The previous generation used a least-square estimator for the age-composition proportions. The proposed changes implemented in this model assume the age-proportion data are logistic-normal, and these data are weighted by the conditional maximum likelihood estimate of the variance (i.e., objectively weighted).  Alternatives likelihood formulations are also easily implemented in future iterations.
          \item Lastly, each catch and survey observation in the input data file also has an associated log standard error associated with it (approximately the coefficient of variation).  In cases where it is possible to estimate a standard error in the data using bootstrap procedures, the inter-annual variation in observation error can now be specified.  In addition, the process error term permits recruitment variation around a stock-recruitment relationship. Currently the Ricker model is implemented, with the option to implement the Beverton-Holt model annotated in the code. 
        \end{itemize}
        \item Additional elements were also introduced in the objective function calculation to improve the overall estimation robustness.  These include penalties that are only implemented in the initial phases to set up initial gradients that will get key population parameters in the ``ball park''. These penalties can then be relaxed (or set = 0) in  the terminal phases.
        \item Of significant difference is the use of informative prior distributions (or sometimes less informative) for population parameters including: natural mortality, initial recruitment, average recruitment, unfished recruitment, steepness of the stock recruitment relationship, and the variance in the recruitment deviations (process error).  The only option for including priors in the previous generation was to fix a parameter value (which implies the variance is 0, or very informative).  For example, having the option to estimate natural mortality where the prior mean is set at the original fixed value and assume some arbitrary CV can often reduce model confounding in cases where there are one-way trips in the relative abundance data. Comparing the marginal posterior density and prior density will shed light on how informative the data are about the parameters.
        \item Model selection criterion can also be evaluated using  Deviance Information Criterion (DIC).  This criterion is calculated using the posterior sample values generated from one of AD Model Builders built-in sampling routines (e.g., The Metropolis Hastings Algorithm).
    \end{itemize}

    Lastly, a few R-scripts have been developed for the purposes of conducting simulation-estimation experiments for self-testing to examine for potential bias in the estimators, and exploring options for correcting any such bias.


  \end{abstract}

  \tableofcontents

  \section{Introduction} % (fold)
  \label{sec:introduction}

  This document describes the structural differences between the Age-structured model for Alaska herring (programmed by Pete Hulson, pete.hulson@noaa.gov). The original model was written in Excel, and translated to the AD Model Builder template languange.  The objective function was a simple weighted least-squares estimator that made numerous simplifying assumptions about the error structure in the data.  The objective for the next generation of stock assessment models for Alaska herring stocks is to bring more modern statistical approach to fitting data collected from fisheries dependent and independent soruces.  

  In this documewnt we first deconstruct the original code to understand how the various observations are related to structural assumptions in the model, and how these data are weighted when estimating model parameters.  Next we describe a technical description of the proposed model changes, and provide both the equations and the AD Model Builder template code to document how the equations are actually implemented in the code.  Having both the equations and the code serve dual purpose, model documentation, and allows for new programmers to learn more about the language itself and implement changes to accomodate alternative structural assumptions.
  
  % section introduction (end)

  \section{Model deconstruction} % (fold)
  \label{sec:model_deconstruction}
  This section is intended to serve three purposes: 1) to document the model structure given the code in model.tpl,  2) to detail proposed changes to the model code to improve overall numerical stability, and 3) provide statistical approach that is amenable to maximum likelihood and Bayesian methods.

    \subsection{Model Structure} % (fold)
    \label{sub:model_structure}
    
    Table \ref{tab:ModelDeconstruction} begins with the objective function that is being minimized.   There are four components defined in \eqref{eq.f}, where three of the four components are scaled by user defined coefficients $\lambda$.  The first component is the commercial age-composition data ($QC$), the second is the spawnig biomass age-composition ($QS$), the third is egg deposition data ($WQE$), and finally the fourth component is a penalty on the recruitment deviations from the underlying Ricker stock-recruitment model ($QR$).

    For the commercial age-composition data, observation errors in the age-proportions are assumed to be normal \eqref{eq.SSQC}, where the predicted proportion-at-age \eqref{eq.Qij} is a function of the numbers-at-age \eqref{eq.Vij} and selectivity \eqref{eq.Sij}. Note that in \eqref{eq.Vij} that the function is not continous. In this case the selectivity curve is rescaled to have a maximum value of 1.0. The \texttt{max} operation in the denominator of this function breaks the derivative chain in AD Model Builder and can result in numerical problems during parameter optimization associated with corrupt derivative information. 

    The same normal error distribution is also assumed for the age-proportions in the spawner catch composition samples \eqref{eq.QS}.  In this case, the age proportions aree based on the mature numbers-at-age at the time of spawning, where the fisehries removals are first subtracted from the mature numbers-at-age \eqref{eq.Oij}.  Note that this further assumes that all removals (i.e., fisheries selectivity) only harvests sexually mature fish.  This assumption is inconsistent with \eqref{eq.Vij}, where a different logistic curve is assumed for fisheries selectivity.  



    \begin{table}[ht]
      \centering
      \caption{Decomposition of the objective function based on the source code provided in \texttt{model.tpl}. The objective function $f$ is what AD Model Builder is trying to minimize. Note that $\dot{}$ represents mature state variables (e.g., mature weight-at-age $\dot{w}_j$)}
      \label{tab:ModelDeconstruction}
      \tableEq
      \begin{align}
        \hline \nonumber
        &\mbox{Objective function} & \nonumber\\
        & f = \lambda_C QC + \lambda_S QS + WQE + \lambda_R QR & \label{eq.f} \\[1ex]
        %%
        &\mbox{Commercial catch proportion-at-age}  \nonumber\\
        & QC = \sum_i \sum_j (\hat{Q}_{i,j} - Q_{i,j})^2 & \mbox{$\hat{Q}_{i,j}$ observed proportions-at-age} \label{eq.SSQC}\\
        & Q_{i,j} = \frac{V_{i,j}}{\sum_j V_{i,j}} & \mbox{$Q_{i,j}$ predicted proportion-at-age} \label{eq.Qij} \\
        & V_{i,j} = N_{i,j} \frac{S_{i,j}}{\max(S_{i,j})} & \mbox{$V_{i,j}$ vulnerable numbers-at-age} \label{eq.Vij}\\
        & S_{i,j} = \frac{1}{1+\exp(-g_i(j-a_i))} &\mbox{$S_{i,j}$ Selectivity in year $i$ for age $j$} \label{eq.Sij} \\[1ex]
        %%
        %%
        &\mbox{Spawn proportion-at-age} \nonumber\\
        & QS = \sum_i \sum_j (\hat{O}_{i,j}-O_{i,j})^2 &  \mbox{$\hat{O}_{i,j}$ observed proportion-at-age} \label{eq.QS}\\
        & O_{i,j} = \frac{\dot{N}_{i,j}}{\sum_j \dot{N}_{i,j} } & \mbox{$O_{i,j}$ predicted proportion mature-at-age} \label{eq.Oij}\\
        & \dot{N}_{i,j} = N_{i,j} M_{i,j} - C_{i,j} & \mbox{$\dot{N}_{ij}$  Number-at-age at spawning} \\
        & C_{i,j} = \frac{\hat{c}_i Q_{i,j}}{\sum_j Q_{i,j} w_j} & \mbox{$\hat{c}_i$ observed catch, $w_j$ weight-at-age} \label{eq.Cij}\\[1ex]
        %%
        %%
        &\mbox{Egg deposition survey} \nonumber\\
        & WQE = \sum_i \varphi_i\left[ \ln(\hat{y}_i)-\ln(y_i) \right]^2 & \mbox{$\hat{y}_i$ observed egg deposition, $\varphi_i$ weight}\label{eq.WQE}\\
        & y_i = 0.5 \sum_j O_{i,j} (\rho_i \dot{w}_{i,j}-\alpha_i) &\mbox{$\rho_i$ and $\alpha_i$ fecundity-weight regression} \\
        %%
        %%
        &\mbox{Penalized recruitment deviations} \nonumber \\
        & QR = \sum_{i}^{(I-k)} \left[  \ln\left(N_{i+k,k}\right) 
        - \ln\left(f(\dot{N}_{i,j}) \right) \right]^2 
          & \mbox{$N_{i+k,k}$ number of age $k$ recruits}\\
        & f(\dot{N}_{i,j}) = a \dot{B}_i \exp(-b \dot{B}_i) &\mbox{Ricker stock-recruitment}\\
        & \dot{B}_i = \sum_j \dot{N}_{i,j} \dot{w}_j &\mbox{$\dot{B}_i$ mature biomass at spawning} \\
      %   %%
        \hline \nonumber
      \end{align}
      \normalEq
    \end{table}
    The catch-at-age data is internally derived in the model \eqref{eq.Cij} conditional on the numbers-at-age and the estimated selectivity. The model further assumes the total catch (in short tons) is measured without error.  This is also referred to as conditioning the model on catch.

    The residual sum of squares for the egg deposition suvey is given by \eqref{eq.WQE}.  In this case the obsrevation errors are assumed to be lognormal, and each year's observation is weighted by the inverse variance of sampling observation errors ($\varphi_i$).

    % subsection model_structure (end)

  % section model_deconstruction (end)


  \clearpage
  \section{Technical description of the proposed model changes} % (fold)
  \label{sec:methods}
  
  \subsection{Input Data} % (fold)
  \label{sub:input_data}
    The best resourcee for looking at the input data is the html file that describes the input data. There are 7 major sections to the data file.
    \begin{enumerate}
      \item Model dimensions.
      \item Fecundity regression coefficients.
      \item Total Annual Catch.
      \item Empirical Weight-at-age (spawn and commercial).
      \item Age-composition (spawn and commercial).
      \item Egg deposition data.
      \item Mile milt days.
    \end{enumerate}

    These are the same data inputs that were used in the ASA model; however, there have a been a number of siginificant changes to the input data file.  The most siginificant change is the addition of the log standar error for each catch, egg deposition, and spawn survey observation.  
  % subsection input_data (end)

  \subsection{Control file} % (fold)
  \label{sub:control_file}
    There are also siginficant changes to the control file.  In fact, its a completely different control file than what was used in the ASA model.  Again, the best resource for looking at the specific contents of the control file, is the control file itself. 

    To highlight some of the major changes, the control file now consists of a design matrix for controling the leading model parameters; specificially, the bounds and phases in which these parameters are estimated.  There is a block for time-varying maturity, a block for time-varying naural mortality rate deviations.  A block for selectivity, where the user can choose among alternative parametric and non-parametric selectivity curves. Lastly, there is a vector of other miscellaneous model controls for, \textit{inter alia}, re-scaling catch, conditioning the model on catch or effort.  

  % subsection control_file (end)

  \subsection{Age-schedule information} % (fold)
  \label{sub:age_schedule_information}
    Age-schedule information is part date input: weight-at-age, or maturity-at-age via regression coefficients. The other part of age-schedule information is estimated based on fitting the model to data: selectivity-at-age, maturity-at-age.  

    \subsubsection{Maturity-at-age} % (fold)
    \label{ssub:maturity_at_age}
      For the maturity-at-age, the \ham\ assumes that age-specific maturity follows a logistic relationship, where the estimated paraemeters define the age-at-50\% maturity and the standard deviation, for each unique block period (the block periods are user defined). 

       The source code for the TPL file is as follows:
      \lstinputlisting[language=C++, firstline=732, lastline=751]{../../src/ham.tpl}
      where \texttt{plogis} is a built in ADMB function for the logistic: \[f(x) =  (1+\exp(-(x-\mu)/\sigma))^{-1} \] where $\mu$ and $\sigma$ are the location and scale parameters that are estimated.
    
    % subsubsection maturity_at_age (end)

    \subsubsection{Natural mortality} % (fold)
    \label{ssub:natural_mortality}
    
      Natural mortality is both age- and time-specific.  At the time of writing, there is no code that allows for age-dependent natural mortality, but this option is easily added as a feature to the \ham.

      The source code for the TPL file is as follows:
      \lstinputlisting[language=C++, firstline=754, lastline=783]{../../src/ham.tpl}
      where the Matrix $M_{i,j}$ is the instananeous natural mortality rate for year $i$ and age $j$.  At this point the code just fills each row of the matrix with the same annual natural mortality rate (i.e., age-independent).  This is where you would want to modify the code to allow for age-dependent natural mortality rates.
    % subsubsection natural_mortality (end)

    \subsubsection{Selectivity} % (fold)
    \label{ssub:selectivity}
      Currently only the logistic selectivity option is implemented. But the source code is structured such that alternative parametric and non-parametric functions can be easily added to the source code using a swtich statement.

       The source code for the TPL file is as follows:
      \lstinputlisting[language=C++, firstline=787, lastline=817]{../../src/ham.tpl}

      The matrix $S_{i,j}$ is the relative selectivity for age $j$ in year $i$.  Additional functions for computing the vector \texttt{slx} can be new cases (e.g. case 2:  // coefficients).

      There is a very important feature in the selectivity is parameterized in this model.  The reason for this particular parameterization is to ensure the objective function remains continuous and differentiable.  In each year, the vector of age-specific selectivity coefficients is scaled to have a mean value of 1.0.  This is accomplished, in log-space, but subtracting the mean from the vector of age-specific selectivities.  This avoids having to use the max function; the use of the max function can lead to a discontinuity in the objective function which result in non-convergence.  The tradeoff for this numerical stability is that the interpretation of Fishing mortality rates changes.  The estimator is calculating the average-age-specific fishing mortality rate.  The fully-selected fishing mortality rate is more commonly used metric, and this is easily accommodated post-estimation by re-scaling the vector of fishing mortality rates by the maximum age-specific selectivity in each year. 

      If the previous paragraph didn't make sense, go read the R-code.
    % subsubsection selectivity (end)

  % subsection age_schedule_information (end)

  \subsection{Fishing mortality} % (fold)
  \label{sub:fishing_mortality}
  If the model is conditioned on effort, then a routine that calculates the age-specific fishing mortality rate is invoked.  In this routine, a vector of fishing mortality rate parametes, in log-space, is estimated, and the age-specific fishing mortality rate is the product of the fishing rate and age-specific selectivity.  The source code for this routine is as follows:
  \lstinputlisting[language=C++, firstline=820, lastline=827]{../../src/ham.tpl}

  If the model is conditioned on catch (i.e., the method the ASA model uses), then there is a resulting difference equation which has the potential to result in negative numbers-at-age, which results in negative Infinity in log-space.  To guard against this, a simple solution might be to use a Max function to ensure that a positive number is returned. However, this is yet another occurance where the objective function is disconituous and subject to non-convergence issues.  AD Model Builder has a function \texttt{posfun} that can be used to ensure the objective function remains continuous and differentiable.
  % subsection fishing_mortality (end)


  \subsection{Population dynamics} % (fold)
  \label{sub:population_dynamics}
  Estimated parameters for the population dynamics model include the initial abundance of ages 3-9+ for the intial year, abundance of age-3 recruits each year, and the natural mortality rate. In the original parameterizeation of the model, these initial recruitments and the vector of initial numbers-at-age result in creating $(N + A-1)$ scaling parameters.  To reduce the potential confounding with other global scaling parmaters, updates to the model code include estimation of two recruitment scaling parameters, and two vectors that represent deviations from the mean. This modification reduces the potential for parameter confounding among the many parameters that affect global scaling (i.e., catchabiltiy coefficients, natural mortlaity rates, average recruitment or unfished biomass).

    
  

  

  \begin{table}[h]
    \centering
    \caption{Notation and equations for population dynamics model.}
    \label{tab:PopulationDynamics}
    \tableEq
    \begin{align}
      \hline \nonumber
      & \mbox{Model parameters} & \nonumber\\
      & \theta = \{\ln(M),\ln(\bar{R}),\ln(\ddot{R}), 
        \ln(\alpha),\ln(\beta),\vec{\delta} \} \\[1ex]
      %%
      & \mbox{Initial States ($i=1980$)} \nonumber \\
      & \iota_j = \begin{cases}
          \exp(-M*(j-\mbox{min}(j))) & \quad \mbox{where } 3 \leq j \leq 7 \\[1ex]
          \dfrac{\exp(-M*(j-\mbox{min}(j)))}{1-\exp(-M)} & \quad \mbox{where } j=8 \\
          \end{cases} & \mbox{survivorship} \label{eq.survivorship}\\[1ex]
      & N_{i,j} = \ddot{R} \exp(\delta_{i-j}) \iota_j \qquad \qquad  i=1980,\forall j & \mbox{initial numbers-at-age} \label{eq.initialnumbersatage} \\
      & N_{i,j} = \bar{R} \exp(\delta_i) \qquad  \quad \qquad \quad \forall i, j=3 & \mbox{age-3 recruits} \label{eq.initialrecruits} \\[1ex]
      %%
      & \mbox{Dynamic States ($i > 1980$)} \nonumber\\
      & Q_{i,j} = \frac{N_{i,j} S_{i,j}}{\sum_j N_{i,j} S_{i,j}} &\mbox{vulnerable proportions} \label{eq.predQij} \\
      & \bar{w}_i  = \sum_j w_j  Q_{i,j} &\mbox{average weight of catch} \\
      & C_{i,j} = \frac{\hat{c}_{i}Q_{i,j}}{\bar{w}_i} \qquad \mbox{where $\hat{c}_i$ is the observed catch (mt)} &\mbox{$C_{i,j}$ catch-at-age} \\
      & \acute{N}_{i,j} = N_{i,j} \exp(-F_{i,j}) \\
      & N_{i+1,j+1} = \acute{N}_{i,j} \exp(-M_{i,j}) \\[1ex]
      %%
      & \mbox{Spawning stock biomass} \nonumber\\
      & B_{i} = \sum_j (N_{ij}-C_{ij}) \omega_{ij} w_j & \mbox{Spawning stock biomass} \label{eq.ssb}\\
      & \mbox{Stock-recruitment} \nonumber\\
      \hline \nonumber
      %%
    \end{align}
    \normalEq
  \end{table}

  \subsubsection{Initial state variables.} % (fold)
    \label{ssub:initial_state_variables_}
    
    In this routine, the objective is to set the initial values for the numbers-at-age matrix $N_{i,j}$.  Specifically, the row dimensions of the matrix are from the start year to the terminal year + 1, the column dimensions are the ages.  This routine first calculates the survivorship to age $j$ based on natural mortality rates, then initializes the first row of $N_{i,j}$ matrix using the average initial recruitment and deviations around that average recruitment multiplied by the survivorship at age $j$.  The source code also includes the taylor series for the plus group:
    \lstinputlisting[language=C++, firstline=831, lastline=851]{../../src/ham.tpl}

    The survivorship calculation \texttt{lx} correspondds to \eqref{eq.survivorship} in Table \ref{tab:PopulationDynamics}, and the numbers-at-age in the initial year and age-3 recruits corresponds to \eqref{eq.initialnumbersatage} and \eqref{eq.initialrecruits}, respectively.
    % subsubsection initial_state_variables_ (end)

    \subsubsection{Update state variables} % (fold)
    \label{ssub:update_state_variables}
    In this routine, the numbers-at-age are propagated in time, where the-age specific survival rate is partitioned into two periods: a fishing period, and a period of natural mortality.  The ASA model currently in use for Sitka sound herring assumes a pulse fishery.  At the start of each time step, the model first calculates the predicted catch-at-age in numbers in year $i$. This is done by first converting the catch weight to catch in numbers by dividing by the predicted average of the catch.  This corresponds to the \texttt{wbar} variable in the following code chunk (note the dependencey on predicted proportions-at-age):
    \lstinputlisting[language=C++, firstline=863, lastline=926]{../../src/ham.tpl}
    Once the catch-at-age data is calculated, the pulse fishery proceeds by subtracting the $C_{ij}$  from the $N_{i,j}$.  The last two steps correspond to step 4 in the annotated code chunk. The last steps the survive the remaining numbers-at-age using the natural mortality rate in year $i$. Then finally, update the numbers-at-age $j$ to $j+1$ in year $i$ to year $i+1$, including the plus group for the terminal age-group.
    % subsubsection update_state_variables (end)

    \subsubsection{Stock-recruitment \& Spawning stock biomass} % (fold)
    \label{ssub:stock_recruitment_&_spawning_stock_biomass}
    The spawning stock biomass (after roe-fishery removal) is the product of the remaining numbers-at-age, the maturity-at-age, and the weight-at-age from spawn samples.  The equation is defined in \eqref{eq.ssb} in Table \ref{tab:PopulationDynamics}.  Note that the lag between spawning biomass and age-3 recruits is taken into account.

    The stock recruitment relationship assumes that recruitment follows a Ricker type.  The form  of the Ricker model is as follows \[ R = s_o B_i \exp(-\beta B_i + \epsilon_i)\] where the parameter $s_o$ is the slope at the origin (or maximum number of recruits per spawning unit), and $\beta$ defines slope of $\ln(R_i/B_i)$ versus the independent variable $B_i$.  These two parameters are derived from the leading parameters the unfished recruitment $R_0$ and the steepness of the stock recruitment relationship as defined by \citep{mace1988generalised}.

    The source code for the stock-recruitment relationship is well annotated and describes some of the derivations:
    \begin{small}
    \lstinputlisting[language=C++, firstline=929, lastline=998]{../../src/ham.tpl}
      
    \end{small}

    There is an issue with regards to calculating reference points in cases where there is non-stationarity (i.e., any time varying parameters such as natural mortality, maturity etc.).  At what period should be used to define the average weight-at-age for spawning herring? What period should be used for calculating the average maturity?  All of these are subjective decisions, and based on my experience will have little impact on the overall fit to the data, but could have major implications for harvest policy changes. 

    % subsubsection stock_recruitment_&_spawning_stock_biomass (end)
  % subsection population_dynamics (end)

  \subsection{Observation models} % (fold)
  \label{sub:observation_models}
    \subsubsection{Age composition} % (fold)
    \label{ssub:age_composition}
    The predicted age-composition is based on the vulnerable proportions at age $Q_{i,j}$ in \eqref{eq.predQij}.  The residual difference is used to compute the negative log likelihood in the objective function.  The code for the age composition residual calculation is as follows:
    \lstinputlisting[language=C++, firstline=1001, lastline=1022]{../../src/ham.tpl}
    Note that both the residual difference between the commercial and spawning samples are calculated in this routine.  If there are missing data for a given year (denoted by a -9.0 in the data file), then the residual difference is set to 0 for that year and there is no contribution to the negative log likelihood.
    % subsubsection age_composition (end)

    \subsubsection{Egg deposition} % (fold)
    \label{ssub:egg_deposition}
    The observation model for the egg deposition data treats these observations as estimates of absolute abundance.  Therefore, there is no associated scaling parameter that is estimated. The observation errors in the egg deposition data are assumed to be lognormal.  The predicted age-deposition data is based on the female (assuming a 50:50 sex ratio) mature numbers-at-age multiplied by the fecundity at age.  The annotated source code is as follows:
    \lstinputlisting[language=C++, firstline=1026, lastline=1041]{../../src/ham.tpl}

    % subsubsection egg_deposition (end)

    \subsubsection{Aerial surveys} % (fold)
    \label{ssub:aerial_surveys}
    Indicies from aerial surveys, or mile milt days, are treated as relative abundance data.  The underlying assumption in this observation model is that the observation errors are log normal, and that the index is proportional to the spawning stock biomass.  Note that the code does not estimate the coefficient, rather the conditional maximum likelihood estimate of the scaling coefficent is used \citep[see][for a full explanation]{walters1994calculation}.  The annotated code follows:
     \lstinputlisting[language=C++, firstline=1043, lastline=1064]{../../src/ham.tpl}
    % subsubsection aerial_surveys (end)

    \subsubsection{Predicted catch} % (fold)
    \label{ssub:predicted_catch}
    In the case where the model is conditioned on effort and fitted to the catch time serie data, the predicted catch is the sum of products betwen the catch-at-age in numbers and the observed weight-at-age in the commercial fishery.
    We further assume observation errors are lognormal. The source code follows:
    \lstinputlisting[language=C++, firstline=1066, lastline=1078]{../../src/ham.tpl}
    % subsubsection predicted_catch (end)
  % subsection observation_models (end)

  \subsection{Objective function} % (fold)
  \label{sub:objective_function}
  The objective function is organized into two sections, the first contains the negative loglikelihoods for the data given the model parameters.  The second are a series of penalties, in the case of maximum likelihood estimation, prior density functions in the case of a Bayesian estimation.
    \subsubsection{Negative loglikelihoods} % (fold)
    \label{ssub:negative_loglikelihoods}
    The negative loglikelihoods 
    % subsubsection negative_loglikelihoods (end)
    There are five 6 negative loglikelihoods functions in the objective function that correspond to the 5 different data elements and the residual process errors associated with a stock-recruitment relationship. Table \ref{tab:likelihoodOptions} summarises the available options currently implemented.

    \begin{table}[h]
      \caption{Data and types of likelihoods implemented}
      \label{tab:likelihoodOptions}
      \begin{tabular}{l|c|c|c|c}
        \hline
        Data & normal & log-normal & multivariate-logistic & multinomial\\
        \hline
        Commercial Age-comps & & & X\\
        Spawn Survey Age-comps & & & X\\
        Egg deposition & & X & \\
        Aerial survey & & X  & \\
        Catch  & & X & \\
        Stock-Recruitment & & X & \\
      \end{tabular}
    \end{table}

    The source code for the negative loglikelihoods follows:
    \lstinputlisting[language=C++, firstline=1079, lastline=1120]{../../src/ham.tpl}
    
    For the composition data, the multivariate logistic likelihood is currently implemented, as this is a self-scaling likelihood.  A good reference for this particular likelihood and how its implemented in AD Model Builder can be found in \cite{schnute1995influence}.  More recent work on the weighting of composition data is available in \cite{francis2011data}.  The function \texttt{dmvlogistic} requires 5 arguments: the observed and predicted composition matrixes, a matrix for returning the residuals, a variable for the conditional MLE of the variance of the observation errors, and a threshold value called \texttt{minp}. The multivariate logistic likelihood does not accommodate 0 observations for age proportions.  Therefore there are two options for dealing with 0s: 1) add a small constant to all observed and predicted observations and re-normalize, or 2) pool the adjacent cohorts if the observed proportion is less than some minimum observed proportion.  The first option is widely used in programs such as stock synthesis, and is akin to manufacturing data.  If a particular cohort is relatively weak, and only partially selected, the sample size required to observed just one individual of a certain age in a given year could be infinitely large.  Instead of adding a constant, option 2 pools the data such that the likelihood of ages, for example, 3-4 are evaluated jointly, rather than the likelihood of age-3 plus the likelihood of age-4.  This pooling of the data and predictions does not require the addition of a constant that could potentially bias the result.  A good practice that I've found is to just set \texttt{minp=0}, and then conduct sensitivity tests using where proportions less than 1\% or 2\% etc are pooled into the adjacent cohort.

    In the case of the likelihoods for the egg deposition index, aerial surveys, and catch data, the function \texttt{dnorm} is used, where the arguments are the vector of residuals, and a vector of standard-deviations for each observation.  Note that you can effectively turn individual years of data off by setting the \texttt{log.se} value to a large number (e.g., 5.0).

    \subsubsection{Penalties} % (fold)
    \label{ssub:penalties}
    Currently the code for the penalties is as follows:
    \lstinputlisting[language=C++, firstline=1124, lastline=1140]{../../src/ham.tpl}

    The penalties are implemented in a phased approach, where in the initial phases of parameter estimation, the penalties initially have small standard deviations. This increases the overall efficiency of the non-linear search routine to help resolve the overall scaling. Then in the terminal phase, these penalties are relaxed (increased variance) such that they provide little or no influence on the gradient structure.  A similar strategy is also used with the recruitment deviation parameters.
    % subsubsection penalties (end)
  % subsection objective_function (end)
\begin{table}
  \centering
  \caption{Mathematical notation, symbols and descriptions.}
  \label{tab:notation}
  \begin{tabular}{cl}
  \hline
  Symbol  & Description \\
  \hline
  \multicolumn{2}{l}{\underline{Index}}\\
      $g$ & group \\
      $h$ & sex \\
      $i$ & year \\
      $j$ & time step (years) \\
      $k$ & gear or fleet \\
      $l$ & index for length class \\
      $m$ & index for maturity state \\
      $o$ & index for shell condition. \\
  \multicolumn{2}{l}{\underline{Leading Model Parameters}}\\
      $M$         & Instantaneous natural mortality rate\\
      $\bar{R}$   & Average recruitment\\
      $\ddot{R}$  & Initial recruitment\\
      $\alpha_r$  & Mode of size-at-recruitment\\
      $\beta_r $  & Shape parameter for size-at-recruitment\\
      $R_0$       & Unfished average recruitment\\
      $\kappa$    & Recruitment compensation ratio\\
  \multicolumn{2}{l}{\underline{Size schedule information}}\\
      $w_{h,l}$   & Mean weight-at-length $l$ \\
      $m_{h,l}$   & Average proportion mature-at-length $l$ \\
  \multicolumn{2}{l}{\underline{Per recruit incidence functions}} \\
      $\phi_B$    & Spawning biomass per recruit \\
      $\phi_{Q_k}$& Yield per recruit for fishery $k$\\
      $\phi_{Y_k}$& Retained catch per recruit for fishery $k$ \\
      $\phi_{D_k}$& Discarded catch per recruit for fishery $k$ \\
  \multicolumn{2}{l}{\underline{Selectivity parameters}} \\
      $a_{h,k,l}$ & Length at 50\% selectivity in length interval $l$\\
      $\sigma_{s_{h,k}}$ & Standard deviation in length-at-selectivity\\
      $r_{h,k,l}$ & Length at 50\% retention\\
      $\sigma_{y_{h,k}}$ & Standard deviation in length-at-retention\\
      $\xi_{h,k}$ & Discard mortality rate for gear $k$ and sex $h$\\
  \hline
  \end{tabular}
\end{table}


  
  \section{Simulation example} % (fold)
  \label{sec:simulation_example}
  
  Built into the assessment model code is a command line option that conducts a simulation experiment.  In this experiment, the input parameter values are known, and simulated observations are generated with known error distributions. The model then proceed to esimate the model parameters using the non-linear optimization procedures.  any potential biases can be detected by comparing the estimated values with the true values. 

  For this simulation example, the data from the Sitka herring stock will be used.


  % section simulation_example (end)


  \section{Example Assessment: Sitka herring} % (fold)
  \label{sec:example_assessment_sitka_herring}
  
  % section example_assessment_sitka_herring (end)


  \bibliographystyle{apalike}
  \bibliography{$HOME/Documents/ARTICLES/Articles-1}

\end{document}
